\chapter{Entropy} \label{chap:entropy}
\section{Overview}
Entropy is an important property used to distinguish measure-preserving transformations from each other and is used extensively in ergodic theory. Chapter \ref{chap:concentration-bounds} looks at methods for estimating entropy and finding inequalities to describe how these `estimators' behave. This chapter focuses on defining entropy and explaining why it is a useful property.

Throughout this chapter $(X, \B, \mu)$ will denote a probability space.

\section{Isomorphisms of measure-preserving transformations}\label{sec:isos-of-mpts}
One of the main problems in ergodic theory is to classify measure-preserving transformations. To this end, we want to decide the conditions required for two measure-preserving transformations to be `the same' -- up to sets of measure zero.

\emph{This section predominantly follows material in \cite[Chapter 2]{walters:intro-to-ergodic-theory}.}

\subsection{Isomorphism and conjugacy of measure spaces}

We begin by defining when two probability spaces are isomorphic or conjugate.

\begin{definition}
	Two probability spaces $(X_1, \B_1, \mu_1), (X_2, \B_2, \mu_2)$ are \key{isomorphic} if there exists $M_1 \in \B_1$, $M_2 \in \B_2$ such that $\mu_1(M_1) = 1 = \mu_2(M_2)$ and if there exists an invertible measure-preserving transformation $\phi: M_1 \to M_2$.
\end{definition}

Let $A, C \subset \B$. We define an equivalence relation on $\B$: we have $A \sim C$ if and only if $\mu(A \symdiff C) = 0$. In other words, $A$ and $C$ belong to the same equivalence class if they are equal almost everywhere. It can be easily checked that $\sim$ is indeed an equivalence relation.

Let $\tilde{\B}$ denote the collection of all equivalence classes in $\B$. Since $\B$ is a $\sigma$-algebra, it is clear that $\tilde{\B}$ is also a $\sigma$-algebra. We can define a measure $\tilde{\mu} : \tilde{\B} \to \reals^+$ by $\tilde{\mu}(\tilde{B}) = \mu(B)$, where $B$ belongs to the equivalence class $\tilde{B}$.

\begin{definition}
	A \key{measure algebra} is a Boolean $\sigma$-algebra equipped with a measure.
\end{definition}

In view of this definition, we see that $(\tilde{\B}, \tilde{\mu})$ is a \key{measure algebra}.

\begin{definition}
	Let $(X_1, \B_1, \mu_1), (X_2, \B_2, \mu_2)$ be probability spaces with corresponding measure algebras $(\tilde{\B}_1, \tilde{\mu}_1), (\tilde{\B}_2, \tilde{\mu}_2)$, respectively.
	
	We say $(\tilde{\B}_1, \tilde{\mu}_1)$ and $(\tilde{\B}_2, \tilde{\mu}_2)$ are \key{isomorphic} if there exists a bijection $\phi : \tilde{\B}_2 \to \tilde{\B}_1$ which preserves complementation and countable unions and intersections such that $\tilde{\mu}_1(\phi \tilde{B}) = \tilde{\mu}_2(\tilde{B})$ for all $\tilde{B} \in \tilde{\B}_2$.
	
	The probability spaces $(X_1, \B_1, \mu_1)$ and $(X_2, \B_2, \mu_2)$ are \key{conjugate} if their corresponding measure algebras are isomorphic.
\end{definition}

\begin{proposition}
	If two probability spaces are isomorphic, then they are also conjugate.
	\begin{proof}
		Suppose $(X_1, \B_1, \mu_1), (X_2, \B_2, \mu_2)$ are isomorphic probability spaces with corresponding measure algebras $(\tilde{\B}_1, \tilde{\mu}_1), (\tilde{\B}_2, \tilde{\mu}_2)$. By definition, this means there exists $M_1 \in \B_1$, $M_2 \in \B_2$ such that $\mu_1(M_1) = 1 = \mu_2(M_2)$ and there exists an invertible measure-preserving transformation $\phi: M_1 \to M_2$.
		
		Now we can define the map
		\[
			\psi : \tilde{\B}_2 \to \tilde{\B}_1 : \tilde{B} \mapsto (\phi^{-1}(M_2 \cap B))^\sim.
		\]
		This is clearly a bijection and, since $\phi$ is measure-preserving and $M_2 = X_2$ almost everywhere, we have
		\[
			\tilde{\mu}_1(\psi\tilde{B}) = \tilde{\mu}_1(\phi^{-1}(M_2 \cap B))^\sim = \tilde{\mu}_2(M_2 \cap B)^\sim = \tilde{\mu}_2(\tilde{B}),
		\]
		for all $\tilde{B} \in \tilde{\B}_2$. Therefore the measure algebras are isomorphic and hence the corresponding measure spaces are conjugate.
	\end{proof}
\end{proposition}

The converse statement is not necessarily true. Indeed, suppose we have the probability space $(X_1, \B_1, \mu_1)$ consisting of exactly one point, and another probability space $(X_2, \B_2, \mu_2)$ consisting of exactly two points, with $\B_2 = \{\emptyset, X_2\}$. It is easy to see that the measure algebras are isomorphic and hence the measure spaces are conjugate.

We need to choose $M_1 \in \B_1$, $M_2 \in \B_2$ such that $\mu_1(M_1) = 1 = \mu_2(M_2)$; the only possibility is $M_1 = X_1$ and $M_2 = X_2$. However there does not exist bijection between these two sets, so the probability spaces are \emph{isomorphic}.

\subsection{A motivational example}
We describe a scenario when two measure-preserving transformations could be considered `the same'. We follow the example in \cite[p58]{walters:intro-to-ergodic-theory}.

We first introduce a new probability space.

\begin{comment}
Let $Y = \{0, 1\}$ and let $(p_0, p_1)$ be a probability vector with no zero entries. Then $(Y, 2^Y, \nu)$ is a measure space, with measure $\nu$ defined by $\nu(y) = p_y$ for $y \in Y$. Now let $X = \{(x_j)_{j = 0}^\infty \mid x_j \in Y\}$, the space of infinite sequences with entries in $Y = \{0, 1\}$.
\end{comment}
\subsubsection{Bernoulli shifts}
Let $Y = \{0, 1, \dots, k - 1\}$ be a set of $k - 1$ symbols and let $p = (p_0, p_1, \dots, p_{k - 1})$ be a probability vector with no zero entries. Let $X = \{(x_j)_{j = 0}^\infty \mid x_j \in Y \text{ for all } j \geq 0\}$ be the space of infinite sequences with entries in $Y$. We may define a measure $\nu$ on cylinders of length $n$ by
\[
	\nu[x_0, x_1, \dots, x_{n - 1}] = p_{x_0} p_{x_1} \dots p_{x_{n - 1}}.
\]
Such measures are known as \key{Bernoulli measures}. Let $\sigma : X \to X$ be the one-sided, left shift map on $X$.

\begin{proposition}
	The measure $\nu$ is $\sigma$-invariant.
	\begin{proof}
		We have
		\begin{align*}
			\nu(\sigma^{-1}[x_1, \dots, x_n]) &= \nu\left(\bigsqcup_{j = 0}^{k - 1}{[j, x_1, \dots, x_n]}\right) \\
				&= \sum_{j = 0}^{k - 1}{\nu[j, x_1, \dots, x_n]} \\
				&= \sum_{j = 0}^{k - 1}{p_j p_{x_1} \dots p_{x_n}} \\
				&= p_{x_1} \dots p_{x_n} \\
				&= \nu[x_1, \dots, x_n].
		\end{align*}
		(We have used the fact that $\sum_{j = 0}^{k - 1}{p_j} = 1$ on the penultimate line.)
	\end{proof}
\end{proposition}

The shift map $\sigma : (X, \nu) \to (X, \nu)$ is called the one-sided $(p_0, p_1, \dots, p_{k - 1})$-shift.

We are now ready to present two measure-preserving transformations which we argue are `the same'.

\subsubsection{The \texorpdfstring{$\mathbf{\left(\frac{1}{2}, \frac{1}{2}\right)}$}{(1/2, 1/2)}-shift and the doubling map}
Let $T : ([0, 1), \B, \mu) \to ([0, 1), \B, \mu) : x \mapsto 2x \bmod 1$ be the doubling map, where $\B$ is the Borel $\sigma$-algebra on $[0, 1)$ and $\mu$ is Lebesgue measure.

Let $\sigma : (X, \C, \nu) \to (X, \C, \nu)$ be the $\left(\frac{1}{2}, \frac{1}{2}\right)$-shift, where
\[
	X := \{(x_j)_{j = 0}^\infty \mid x_j \in \{0, 1\} \text{ for all } j \geq 0\},
\]
$\C$ is the $\sigma$-algebra generated by all cylinders in $X$, and $\nu$ is the Bernoulli measure as described above with $p = \left(\frac{1}{2}, \frac{1}{2}\right)$.

Define the map $\phi : X \to [0, 1)$ by
\[
	\phi(x_0, x_1, \dots) = \sum_{j = 0}^\infty{\frac{x_j}{2^{j + 1}}} = \frac{x_0}{2^1} + \frac{x_1}{2^2} + \frac{x_2}{2^3} + \dots.
\]
It is easy to see that $\phi$ maps the binary expansion of a number to the actual number itself.

Let $E := \{(x_j)_{j = 0}^\infty \in X \mid (x_j)_{j = N}^\infty \text{ is constant for some } N \geq 0\}$ be the set of sequences in $X$ whose coordinates are eventually constant. Now, if the binary expansion of a number is \emph{not} eventually constant, then this binary expansion is unique. Therefore $\phi$ is \emph{injective} on $X \setminus E$. It is also clear that $\phi$ is \emph{surjective}, since every number in $[0, 1)$ has at least one binary expansion. In addition, it is easy to that $\phi \circ \sigma = T \circ \phi$.

We now show that $\phi$ is measure-preserving. A dyadic interval is an interval of the form $\left[\frac{a}{2^s}, \frac{a + 1}{2^s}\right] \subset [0, 1)$, where $s \in \naturals$. We can write
\[
	\frac{a}{2^s} = \sum_{j = 0}^{s - 1}{\frac{a_j}{2^j}} \quad \text{and} \quad \frac{a + 1}{2^s} = \sum_{j = 0}^\infty{\frac{a_j}{2^j}},
\]
where $a_j \in \{0, 1\}$ for $j = 0, 1, \dots, s - 2$ and $a_k = 1$ for $k \geq s - 1$. In other words, the binary expansion of all numbers in the interval $\left[\frac{a}{2^s}, \frac{a + 1}{2^s}\right]$ agree in the first $s$ positions. Thus,
\begin{align*}
	\nu\left(\phi^{-1}\left[\frac{a}{2^s}, \frac{a + 1}{2^s}\right]\right) &= \nu[a_0, a_1, \dots, a_{s - 1}] \\
		&= \frac{1}{2^s} \\
		&= \mu\left[\frac{a}{2^s}, \frac{a + 1}{2^s}\right].
\end{align*}
Hence $\phi$ is measure-preserving on dyadic intervals, which generate the Borel $\sigma$-algebra $\B$ on $[0, 1)$. We may therefore apply the Kolmogorov Extension Theorem and it follows that $\phi$ is \emph{measure-preserving} on all Borel sets $B \in \B$.

Let $D := \left\{\frac{a}{2^s} \in [0, 1) \mid s \in \naturals,\ 0 \leq a < 2^s\right\}$ be the set of all dyadic rationals in $[0, 1)$. Clearly, $T^{-1}D = D$ and this means that $T^{-1}([0, 1) \setminus D) = [0, 1) \setminus D$. It is also clear that $\sigma^{-1}E = E$ and so $\sigma^{-1}(X \setminus E) = X \setminus E$. So by the above observations, we see that $\phi: X \setminus E \to [0, 1) \setminus D$ is a bijection. It is also clear that $\phi \circ \sigma(x) = T \circ \phi(x)$ for all $x \in X \setminus E$.

Finally, we have $D \subset \rationals$ which gives $\mu(D) = 0$, and we also note that there are countably many sequences in $E$, thus $\nu(E) = 0$. Therefore $\phi$ is an invertible measure-preserving transformation between $X$ and $[0, 1)$ (modulo sets of measure zero), that is, the measure-preserving transformations are \emph{isomorphic}. Therefore it makes sense to say that these measure-preserving transformations are `the same'.

\subsection{\texorpdfstring{\sloppy Isomorphism and conjugacy of measure-preserving transformations}{Isomorphism and conjugacy of measure-preserving transformations}}
We now formalise the ideas illustrated in the above example.

\begin{definition}
	\sloppy Let $(X_1, \B_1, \mu_1, T_1)$, $(X_2, \B_2, \mu_2, T_2)$ be measure-preserving transformations of probability spaces. We say that $T_1$ is \key{isomorphic} to $T_2$ if there exists $M_1 \in \B_1$, $M_2 \in \B_2$ such that $\mu_1(M_1) = 1 = \mu_2(M_2)$ with
	\begin{enumerate}
		\item $T_1{M_1} \subset M_1$ and $T_2{M_2} \subset M_2$, and \label{mpt-iso-i}
		\item there exists an invertible measure-preserving transformation $\phi : M_1 \to M_2$ such that $\phi \circ T_1(x) = T_2 \circ \phi(x)$ for all $x \in M_1$. \label{mpt-iso-ii}
	\end{enumerate}
	If this is the case, we write $T_1 \simeq T_2$.
\end{definition}

Now suppose that $T_1 \simeq T_2$ with $M_1$, $M_2$ and $\phi : M_1 \to M_2$ as in the above definition. Then for $n \geq 1$ we clearly have $T_1^n{M_1} \subset M_1$ and $T_2^n{M_2} \subset M_2$, satisfying condition \ref{mpt-iso-i}. This in turn gives that $\phi \circ T_1^n(x) = T_2^n \circ \phi(x)$ for all $x \in M_1$, satisfying condition \ref{mpt-iso-ii}. In other words, if $T_1 \simeq T_2$, then $T_1^n \simeq T_2^n$ for all $n \geq 1$.

We also have the notion of conjugacy of measure-preserving transformations.

\begin{definition}
	Let $(X_1, \B_1, \mu_1, T_1)$, $(X_2, \B_2, \mu_2, T_2)$ be measure-preserving transformations of probability spaces. We say that $T_1$ is \key{conjugate} to $T_2$ if there exists an isomorphism $\Phi : (\tilde{\B}_2, \tilde{\mu}_2) \to (\tilde{\B}_1, \tilde{\mu}_1)$ of measure algebras such that $\Phi \circ \tilde{T}_2^{-1} = \tilde{T}_1^{-1} \circ \Phi$.
\end{definition}

It can be easily checked that isomorphism and conjugacy are equivalence relations on the set of all measure-preserving transformations.

As with probability spaces, isomorphic measure-preserving transformations are also conjugate. We show this in the following result.

\begin{theorem}\label{thm:walters-2-5}
	Let $(X_1, \B_1, \mu_1, T_1)$, $(X_2, \B_2, \mu_2, T_2)$ be measure-preserving transformations of probability spaces and suppose that $T_1 \simeq T_2$. Then $T_1$ is conjugate to $T_2$.
	
	\begin{proof}
		Suppose that $T_1 \simeq T_2$, so there exists a measure-preserving transformation $\phi : M_1 \to M_2$ such that $\phi \circ T_1(x) = T_2 \circ \phi(x)$ for all $x \in M_1$, where $M_1, M_2$ are as in the definition.
		
		Define $\Phi : (\tilde{\B}_2, \tilde{\mu}_2) \to (\tilde{\B}_1, \tilde{\mu}_1)$ by $\Phi(\tilde{B}) \mapsto (\phi^{-1}(B \cap M_2))^\sim$ for $B \in B_2$. Recall that $\tilde{B}$ is an equivalence class, so it is easy to see that $\Phi$ is an isomorphism. We also have
		\[
			\tilde{T}_1^{-1} \circ \Phi(\tilde{B}) = \tilde{T}_1^{-1} \circ (\phi^{-1}(B \cap M_2))^\sim = \phi^{-1} \circ \tilde{T}_2^{-1} (B \cap M_2)^\sim = \Phi \circ \tilde{T}_2^{-1}(B)
		\]
		for all $B \in \B_2$. Hence $T_1$ is conjugate to $T_2$.
	\end{proof}
\end{theorem}

The converse of this theorem is not necessarily true. However, we will find it useful to know the conditions for which the converse holds. We need the following definition from \cite[Definition A.21]{einsiedler-ward:ergodic-nt}.

\begin{definition}
	Let $Y$ be a set of countably or finitely many points, where each $y \in Y$ has positive measure $p_y > 0$ such that $\sum_{y \in Y}{p_y} \leq 1$. Put $s := 1 - \sum_{y \in Y}{p_y}$ and let $\mathcal{L}[0, s]$ denote the $\sigma$-algebra of Lebesgue measurable sets on the closed interval $[0, s]$. Let $\lambda_{[0, s]}$ denote Lebesgue measure on $[0, s]$.
	
	If the probability space $(X, \B, \mu)$ is isomorphic to the probability space
	\[
		\left([0, s] \sqcup Y,\ \mathcal{L}[0, s],\ \lambda_{[0, s]} + \sum_{y \in Y}{p_y \delta_y} \right),
	\]
	where $\delta_y$ is the Dirac measure at $y$, then we say that $(X, \B, \mu)$ is a \key{Lebesgue space}.
\end{definition}

We will also use the following result, which is proved in \cite[Theorem 12]{royden:real-analysis}.

\begin{lemma} \label{lem:walters-thm-2-2}
	For $j = 1, 2$, let $(X_j, \B(X_j), \mu_j)$ be complete separable metric spaces endowed with Borel $\sigma$-algebra $\B(X_j)$ and probability measure $\mu_j$. Suppose that $\Phi: \tilde{\B}(X_2) \to \tilde{\B}(X_1)$ is an isomorphism of measure algebras. Then there exists $M_1 \in \B(X_1)$, $M_2 \in \B(X_2)$ such that $\mu_1(M_1) = 1 = \mu_2(M_2)$, and an invertible measure-preserving transformation $\phi: M_1 \to M_2$ such that $\Phi(\tilde{B}) = (\phi^{-1}(B \cap M_2))^\sim$ for all $B \in \B(X_2)$.
	
	If $\psi$ is any other isomorphism $(X_1, \B(X_1), \mu_1)$ to $(X_2, \B(X_2), \mu_2)$ which induces $\Phi$, then $\mu_1\{x \in X_1 \mid \phi(x) \neq \psi(x)\} = 0$.
\end{lemma}

The following result gives the conditions for which the converse of \thref{thm:walters-2-5} is true.

\begin{theorem} \label{thm:walters-2-6}
	Suppose that either $(X_1, \B_1, \mu_1)$, $(X_2, \B_2, \mu_2)$ are Lebesgue spaces, or that $X_1, X_2$ are each complete separable metric spaces with corresponding Borel $\sigma$-algebras $\B_1, \B_2$. Suppose that $T_1 : X_1 \to X_1$, $T_2 : X_2 \to X_2$ are measure-preserving transformations and that $T_1$ is conjugate to $T_2$. Then $T_1 \simeq T_2$.
	\begin{proof}
		Suppose that $\Phi : (\tilde{B}_2, \tilde{\mu}_2) \to (\tilde{B}_1, \tilde{\mu}_1)$ is an isomorphism of measure algebras such that $\Phi \circ \tilde{T}_2^{-1} = \tilde{T}_1^{-1} \circ \Phi$. By \thref{lem:walters-thm-2-2} there exists sets $X'_1 \in \B_1$, $X'_2 \in \B_2$ such that $\mu_1(X'_1) = 1 = \mu(X'_2)$, and there exists an invertible measure-preserving transformation $\phi : X'_1 \to X'_2$ such that $\Phi(\tilde{B}) = (\phi^{-1}(B \cap X'_2))^\sim$ for all $B \in \B_2$. Then we have $\tilde{\phi}^{-1} \circ \tilde{T}_2^{-1} = \tilde{T}_1^{-1} \circ \tilde{\phi}^{-1}$, i.e. $T_2 \circ \phi = \phi \circ T_1$ almost everywhere.
		
		Now put
		\[
			A_1 := \{x \in X_1 \mid T_2 \circ \phi(x) = \phi \circ T_1(x)\} \quad \text{and} \quad M_1 := \bigcap_{n = 0}^\infty{T_1^{-n}{A_1}}.
		\]
		Then $\mu_1(M_1) = 1$ and $T_1^{-1}{M_1} \supset M_1$ which means that $M_1 \supset T_1 M_1$. We then define $M_2 := \phi M_1$ so that $T_2 M_2 \subset M_2$. Hence $T_1 \simeq T_2$.
	\end{proof}
\end{theorem}

As we mentioned briefly at the beginning of Section \ref{sec:isos-of-mpts}, we want to be able to decide when two measure-preserving transformations are `the same'. In view of the above discussion, `the same' can be replaced with `conjugate' or `isomorphic'. \key{Entropy} is one of the main conjugacy and isomorphism invariants studied in ergodic theory, and the remainder of this chapter will describe how the entropy of a measure-preserving transformation is defined.

The rest of this chapter predominantly follows \cite[Chapter 4]{walters:intro-to-ergodic-theory} unless otherwise stated. In particular, any definitions relating to \emph{information} is derived from \cite[p33-34]{parry-pollicott:zeta-fns-periodic-orbits}

\section{Entropy of partitions and sub-\texorpdfstring{$\sigma$}{sigma}-algebras}
\subsection{Partitions and sub-\texorpdfstring{$\sigma$}{sigma}-algebras}

We begin with a finite partition $\alpha = \{A_1, \dots, A_m\}$ of $(X, \B, \mu)$, i.e. the $A_j$ are pairwise disjoint and $X = \bigsqcup_{j = 1}^m{A_j}$. For clarity, we will denote partitions by the Greek letters, usually $\alpha, \beta$ or $\gamma$. Consider the collection of all elements of $\B$ such that their unions are elements of $\alpha$. Such a collection is a sub-$\sigma$-algebra of $\B$ and we will denote it by $\A(\alpha)$.

On the other hand, consider a finite sub-$\sigma$-algebra $\C = \{C_1, \dots, C_n\}$ of $\B$. We will use script uppercase letters to denote sub-$\sigma$-algebras, usually $\A, \C$ or $\D$. We can form a partition of $X$ by $\{B_1, \dots, B_n\}$, where $B_j = C_j$ or $X \setminus C_j$. We denote this partition by $\alpha(\C)$.

Note that if $\C$ is a sub-$\sigma$-algebra of $\B$ and $\gamma$ is a partition of $X$, then $\A(\alpha(\C)) = \C$ and $\alpha(\A(\gamma)) = \gamma$. This means that there is a one-to-one correspondence between finite partitions of $X$ and finite sub-$\sigma$-algebras of $\B$. Hence, in a lot of cases, we may use ``partition'' and ``sub-$\sigma$-algebra'' interchangeably.

If $T: X \to X$ is a measure-preserving transformation and $n \geq 0$, then $T^{-n}{\alpha}$ denotes the partition $\{T^{-n}{A_1}, \dots, T^{-n}{A_k}\}$.

\begin{remark}
	Let $\alpha = \{A_1, \dots, A_m\}$ be a finite partition of $(X, \B, \mu)$. Throughout this chapter, we may assume without loss of generality that $\mu(A_j) > 0$ for all $j = 1, \dots, m$. Indeed, we may index $\alpha$ so that
	\[
		\mu(A_j)
		\begin{cases}
			> 0, & \text{if } 1 \leq j \leq p; \\
			= 0, & \text{if } p + 1 \leq j \leq m.
		\end{cases}
	\]
	Then we may form a new partition $\alpha'$ consisting of the sets $A_1, \dots, A_{p - 1}$ and $\bigsqcup_{j = p}^m{A_j}$. Clearly, the disjoint union has the same measure as $A_p$ and so all the sets in $\alpha'$ have strictly positive measure.
	
	This argument can be easily modified for countable partitions.
\end{remark}

\begin{definition}
	Suppose that $\alpha, \gamma$ are finite partitions of $(X, \B, \mu)$. If each element of $\alpha$ can be written as the union of elements of $\gamma$, then we write \key{$\alpha \leq \gamma$}. In particular, we have $\alpha \leq \gamma$ if and only if $\A(\alpha) \subset \A(\gamma)$, and $\A \subset \C$ if and only if $\alpha(\A) \leq \alpha(\C)$.
\end{definition}

\begin{definition}
	Let $\alpha = \{A_1, \dots, A_m\}$, $\gamma = \{C_1, \dots, C_n\}$ be two finite partitions of a measure space $(X, \B, \mu)$. We define their \key{join} $\alpha \join \gamma$ as the partition
	\[
		\alpha \join \gamma := \{A_j \cap C_k \mid 1 \leq j \leq m, 1 \leq k \leq n\}.
	\]
	If $\A, \C$ are finite sub-$\sigma$-algebras of $\B$, then we define the join $\A \join \C$ in the same way. If this is the case, then $\A \join \C$ is actually the smallest sub-$\sigma$-algebra of $\B$ containing both $\A$ and $\C$.
	
	It is clear that $\A \join \C$ is comprised of the unions of sets of the form $A \cap C$, where $A \in \A, C \in \C$.
	
	We also have the relations $\alpha(\A \join \C) = \alpha(\A) \join \alpha(\C)$ and $\A(\alpha \join \gamma) = \A(\alpha) \join \A(\gamma)$.
\end{definition}

\begin{remark}
	If $T : X \to X$ is a measure-preserving transformation and $n \geq 0$, then $T^{-n}$ preserves set theoretic operations and so we have
	\begin{enumerate}
		\item $\alpha(T^{-n}{\A}) = T^{-n}{\alpha(\A)}$,
		\item $\A(T^{-n}{\alpha}) = T^{-n}{\A(\alpha)}$,
		\item $T^{-n}(\A \join \C) = T^{-n}{\A} \join T^{-n}{\C}$,
		\item $T^{-n}(\alpha \join \gamma) = T^{-n}{\alpha} \join T^{-n}{\gamma}$,
		\item if $\alpha \leq \gamma$, then $T^{-n}{\alpha} \leq T^{-n}{\gamma}$,
		\item if $\A \subset \C$, then $T^{-n}{\A} \subset T^{-n}{\C}$.
	\end{enumerate}
\end{remark}

\begin{definition}
	Let $\alpha, \gamma$ be two partitions of $(X, \B, \mu)$. We say that $\alpha$ and $\gamma$ are \key{independent} if $\mu(A \cap C) = \mu(A)\mu(C)$ for all $A \in \alpha$, $C \in \gamma$.
\end{definition}

\subsection{Motivation for information and entropy}
The following motivation for information and entropy follows that of \cite[Lecture 23]{ergodic-lectures}.

Suppose that we want to locate a point $x \in X$. To do this, we can partition the state space $X$ by the finite partition $\alpha = \{A_1, \dots, A_k\}$. We will later show that we may also consider countable partitions. If we find that $x \in A_j$, then we have received some \key{information}, and we think of $\mu(A_j)$ to be the probability that this happens.

We would like to define a function $I_\mu(\alpha) : X \to \reals^+$ such that $I_\mu(\alpha)(x)$ is the information received upon observing that $x \in A_j$. We want $I_\mu(\alpha)$ to only depend on $\mu(A_j)$, and in particular, we should receive more information if $\mu(A_j)$ is small, and we should receive less information if $\mu(A_j)$ is large. So we want $I_\mu(\alpha)$ to be of the form
\[
	I_\mu(\alpha)(x) = \sum_{A \in \alpha}{\chi_A(x)\phi(\mu(A))},
\]
where $\phi : [0, 1] \to \reals^+$ is some nonnegative function.

For two independent partitions $\alpha, \gamma$, the information gained from observing that $x \in A \cap C$, where $A \in \alpha, C \in \gamma$, should be equal to the information we gain from observing $x \in A$ in addition to observing $x \in C$. In view of this, we would require that $I_\mu(\alpha \join \gamma) = I_\mu(\alpha) + I_\mu(\gamma)$.

Combining the above requirements, we get that $\phi(\mu(A \cap C)) = \phi(\mu(A)\mu(C)) = \phi(\mu(A)) + \phi(\mu(C))$. For $\phi$ to be a continuous function, we see that $\phi(t)$ must be a multiple of $-\log{t}$. This gives rise to the following definitions.

\subsection{Information and entropy of partitions}
\begin{definition}
	Let $\alpha$ be a partition of $(X, \B, \mu)$. We define the \key{information} $I_\mu(\alpha) : X \to \reals^+$ of the partition $\alpha$ (or of the sub-$\sigma$-algebra $\A(\alpha)$) by
	\[
		I_\mu(\A(\alpha))(x) = I_\mu(\alpha)(x) := -\sum_{A \in \alpha}{\chi_A(x) \log{\mu(A)}}.
	\]
	We define the \key{entropy} $H_\mu(\alpha)$ of the partition $\alpha$ (or of the sub-$\sigma$-algebra $\A(\alpha)$) to be the average of the information, i.e.
	\begin{align*}
		H_\mu(\A(\alpha)) = H_\mu(\alpha) &:= \int{I_\mu(\alpha)\ d\mu} \\
			&= \int{-\sum_{A \in \alpha}{\chi_A \log{\mu(A)}}\ d\mu} \\
			&= -\sum_{A \in \alpha}{\mu(A) \log{\mu(A)}}.
	\end{align*}
	Whenever we use this definition and those derived from it, we will use the convention that $x \log x = 0$ if $x = 0$.
\end{definition}

\begin{remark}
	If $\alpha = \{X, \emptyset\}$, then we don't gain any information from performing observations on this partition, so $H(\alpha) = 0$. This can also be easily verified from the definition above.
\end{remark}

It is useful to know that, given a partition of $(X, \B, \mu)$ into $k$ sets, we can find an upper bound for the entropy of the partition.

\begin{proposition} \label{prop:walters-cor-4-2-1}
	Let $\alpha = \{A_1, \dots, A_k\}$ be a partition of $(X, \B, \mu)$ into $k$ sets. Then $H_\mu(\alpha) \leq \log{k}$.
	
	In particular, we have $H_\mu(\alpha) = \log{k}$ if and only if $\mu(A_j) = 1 / k$ for all $j = 1, \dots k$.
	
	\begin{proof}
		By \thref{thm:walters-4-2-xlogx-convex}, $x \log{x}$ is strictly convex. This means that for any partition $\alpha = \{A_1, \dots, A_k\}$ of $(X, \B, \mu)$ and for any $\{\lambda_j \in [0, 1] \mid j \in \{1, \dots, k\},\ \sum_{j = 1}^k{\lambda_j} = 1\}$, we have
		\[
			\left(\sum_{j = 1}^k{\lambda_j \mu(A_j)}\right) \log{\left(\sum_{j = 1}^k{\lambda_j \mu(A_j)}\right)} \leq \sum_{j = 1}^k{\lambda_j \mu(A_j) \log{\mu(A_j)}},
		\]
		with equality if and only if $\mu(A_1) = \mu(A_2) = \dots = \mu(A_k)$ whenever $\lambda_j \neq 0$ for all $j = 1, \dots, k$.
		
		Substituting in $\lambda_j = 1 / k$ for all $j = 1, \dots, k$ and rearranging, we get
		\[
			H_\mu(\alpha) = -\sum_{j = 1}^k{\mu(A_j) \log{\mu(A_j)}} \leq -\log{\frac{1}{k}} = \log{k},
		\]
		with equality if and only if $\mu(A_j) = 1 / k$ for all $j = 1, \dots, k$.
	\end{proof}
\end{proposition}

\section{Conditional entropy}
\subsection{Conditional expectation}
The definitions and results in this subsection follow those in \cite[p8-9]{walters:intro-to-ergodic-theory}.
\begin{definition}
	Suppose that $\mu$, $\nu$ are probability measures on a measurable space $(X, \B)$. If all sets $B \in \B$ with $\mu$-measure zero are also sets of $\nu$-measure zero, then we say that $\nu$ is \key{absolutely continuous} with respect to $\mu$. If this is the case, we write $\nu \ll \mu$.
	
	Stated alternatively, we have $\nu \ll \mu$ if, for all $B \in \B$ such that $\mu(B) = 0$, then $\nu(B) = 0$.
	
	Note that there may be more sets of $\nu$-measure zero. In the case where $\nu \ll \mu$ and $\mu \ll \nu$, we say that $\mu$ and $\nu$ are \key{equivalent}.
\end{definition}

\begin{theorem}[Radon-Nikodym Theorem] \label{thm:radon-nikodym}
	Suppose that $\mu, \nu$ are probability measures on a measurable space $(X, \B)$. Then $\nu \ll \mu$ if and only if there exists a nonnegative $\mu$-integrable function $f \in L^1(X, \B, \mu)$ where $f \geq 0$, $\int{f\ d\mu} = 1$, such that $\nu(B) = \int_B{f\ d\mu}$ for all $B \in \B$.
	
	Moreover, the function $f$ is unique almost everywhere, i.e. if there exists another function $g$ which satisfies the above properties, then $f = g$ $\mu$-almost everywhere.
\end{theorem}

The Radon-Nikodym Theorem allows us to define the conditional expectation operator.

\begin{definition}
	Let $(X, \B, \mu)$ be a measure space and let $\C$ be a sub-$\sigma$-algebra of $\B$. The \key{conditional expectation} operator $E_\mu(\seedot \mid \C) : L^1(X, \B, \mu) \to L^1(X, \C, \mu)$ is defined as follows.
	
	If $f \in L^1(X, \B, \mu)$ is a nonnegative real-valued integrable function, then
	\[
		\nu_f(C) = a^{-1}\int_C{f\ d\mu},
	\]
	for $C \in \C$, where $a = \int_X{f\ d\mu}$, defines a probability measure $\nu_f$ on $(X, \C)$ with $\nu_f \ll \mu$. By \thref{thm:radon-nikodym}, there exists a nonnegative function $E_\mu(f \mid \C) \in L^1(X, \C, \mu)$ such that $\int_C{E_\mu(f \mid \C)\ d\mu} = \int_C{f\ d\mu}$ for all $C \in \C$. Furthermore, $E_\mu(f \mid \C)$ is unique almost everywhere.
	
	If $f$ is a real-valued function, we consider the positive and negative parts of $f = f^+ - f^-$, where $f^+, f^- \geq 0$, and define $E_\mu(f \mid \C) := E_\mu(f^+ \mid \C) - E_\mu(f^- \mid \C)$.
	
	If $f$ is complex-valued, we take the real and imaginary parts of $f$ and define $E_\mu(f \mid \C)$ linearly as above.
\end{definition}

The conditional expectation operator $E_\mu(f \mid \C)$ is uniquely determined by the requirement that $E_\mu(f \mid \C)$ is $\C$-measurable, and also that
\[
	\int_C{f\ d\mu} = \int_C{E_\mu(f \mid \C)\ d\mu},
\]
for all $C \in \C$. With this in mind, we can think of $E_\mu(f \mid \C)$ as the best approximation of $f$ in the smaller space $\C$ of measurable functions.~\cite[Lecture 21]{ergodic-lectures}

\subsubsection{Properties of \texorpdfstring{$E_\mu(\seedot \mid \C)$}{the conditional expectation operator}}
\begin{enumerate}
	\item Conditional expectation $E_\mu(\seedot \mid \C)$ is a linear operator. \label{cond-exp:1}
	\item If $f \geq 0$, then $E_\mu(f \mid \C)$. \label{cond-exp:2}
	\item If $f \in L^1(X, \B, \mu)$ and $g$ is a $\C$-measurable bounded function, then $E_\mu(fg \mid \C) = gE_\mu(f \mid \C)$. \label{cond-exp:3}
	\item For $f \in L^1(X, \B, \mu)$, we have $\left|E_\mu(f \mid \C)\right| \leq E_\mu(|f| \mid \C)$. \label{cond-exp:4}
	\item If $\C_2 \subset \C_1$, then for $f \in L^1(X, \B, \mu)$, we have $E_\mu(E_\mu(f \mid \C_1) \mid \C_2) = E_\mu(f \mid \C_2)$. \label{cond-exp:5}
\end{enumerate}

If $f$ is an integrable function, then we can find $E_\mu(f \mid \C)$ using the following formula.

\begin{proposition}
	Let $\C$ be a finite or countable sub-$\sigma$-algebra of $\B$. Then
	\[
		E_\mu(f \mid \C)(x) = \sum_{C \in \gamma}{\int_{C}{f\ d\mu}\frac{\chi_{C}(x)}{\mu(C)}}.
	\]
	
	\begin{proof}
		We follow the proof given in \cite[Example 10.1.2]{bogachev:measure}.
		
		The summation is clearly an integrable function, and the $\C$-measurable functions are exactly the characteristic functions of $C \in \C$. Therefore the result is equivalent to
		\[
			\int{\chi_B(x) \cdot E_\mu(f \mid \C)(x)\ d\mu} = \int{\left(\chi_B(x) \cdot \sum_{C \in \gamma}{\int_{C}{f\ d\mu}\frac{\chi_{C}(x)}{\mu(C)}}\right)\ d\mu},
		\]
		for any $B \in \C$. This is clearly true since by definition,
		\[
			\int{\chi_B(x) \cdot E_\mu(f \mid \C)(x)\ d\mu} = \int_B{f\ d\mu},
		\]
		and
		\[
			\int{\left(\chi_B(x) \cdot \sum_{C \in \gamma}{\int_{C}{f\ d\mu}\frac{\chi_{C}(x)}{\mu(C)}}\right)\ d\mu} = \int{\left({\int_{B}{f\ d\mu}\frac{\chi_{B}(x)}{\mu(B)}}\right)\ d\mu} = \int_{B}{f\ d\mu}.
		\]
	\end{proof}
\end{proposition}

\begin{definition}
	Let $\C \subset \B$ be a sub-$\sigma$-algebra of a $\sigma$-algebra $\B$. The \key{conditional probability} of $B \in \B$ given $\C$ is defined
	\[
		\mu(B \mid \C) := E_\mu(\chi_B \mid \C).
	\]
\end{definition}

\subsection{Conditional information and entropy}
We can define the conditional information and entropy of a partition $\alpha$, given that we know the information gained from the partition $\gamma$. Conditional entropy will prove to be useful later when look at the entropy of measure-preserving transformations.

\begin{definition}
	Let $(X, \B, \mu, T)$ be a measure-preserving transformation on a probability space. Let $\A, \C$ be finite sub-$\sigma$-algebras, where $\alpha(\A) = \{A_1, \dots, A_p\}$, $\alpha(\C) = \{C_1, \dots, C_q\}$. The \key{conditional entropy} of $\alpha$ given $\C$ is defined
	\begin{align*}
		H_\mu(\alpha(\A) \mid \alpha(\C)) = H_\mu(\A \mid \C) &:= -\sum_{k = 1}^q{\mu(C_k) \sum_{j = 1}^{p}{\frac{\mu(A_j \cap C_k)}{\mu(C_k)} \log{\frac{\mu(A_j \cap C_k)}{\mu(C_k)}}}} \\
			&= -\sum_{j, k}{\mu(A_j \cap C_k) \log{\frac{\mu(A_j \cap C_k)}{\mu(C_k)}}}.
	\end{align*}
\end{definition}

\begin{remark}
	If $\mathcal{N} = \{X, \emptyset\}$, then we have $H_\mu(\alpha \mid \mathcal{N}) = H_\mu(\alpha)$. Again, this is because we gain no information from $\mathcal{N}$.
\end{remark}

\begin{theorem} \label{thm:walters-4.3}
	Let $(X, \B, \mu)$ be a probability space and let $\A, \B, \D$ be finite sub-$\sigma$-algebras of $\B$. Suppose $T : X \to X$ is a measure-preserving transformation. Then
	\begin{enumerate}
		\item $H_\mu(\A \join \C \mid \D) = H_\mu(\A \mid \D) + H_\mu(\C \mid \A \join \D)$, \label{walters-thm-4.3:1}
		\item $H_\mu(\A \join \C) = H_\mu(\A) + H_\mu(\C \mid \A)$, \label{walters-thm-4.3:2}
		\item if $\A \subset \C$, then $H_\mu(\A \mid \D) \leq H_\mu(\C \mid \D)$, \label{walters-thm-4.3:3}
		\item if $\A \subset \C$, then $H_\mu(\A) \leq H_\mu(\C)$, \label{walters-thm-4.3:4}
		\item if $\C \subset \D$, then $H_\mu(\A \mid \C) \geq H_\mu(\A \mid \D)$, \label{walters-thm-4.3:5}
		\item $H_\mu(\A) \geq H_\mu(\A \mid \D)$, \label{walters-thm-4.3:6}
		\item $H_\mu(\A \join \C \mid \D) \leq H_\mu(\A \mid \D) + H_\mu(\C \mid \D)$, \label{walters-thm-4.3:7}
		\item $H_\mu(\A \join \C) \leq H_\mu(\A) + H_\mu(\C)$, \label{walters-thm-4.3:8}
		\item $H_\mu(T^{-1}\A \mid T^{-1}\C) = H_\mu(\A \mid \C)$, \label{walters-thm-4.3:9}
		\item $H_\mu(T^{-1}\A) = H_\mu(\A)$. \label{walters-thm-4.3:10}
	\end{enumerate}
	\begin{proof} \hfill
		\begin{enumerate}
			\item We have
				\[
					H_\mu(\A \join \C \mid \D) = -\sum_{j, k, m}{\mu(A_j \cap C_k \cap D_m) \log{\frac{\mu(A_j \cap C_k \cap D_m)}{\mu(D_m)}}}.
				\]
				If $\mu(A_j \cap D_m) \neq 0$, then
				\[
					\frac{\mu(A_j \cap C_k \cap D_m)}{\mu(D_m)} = \frac{\mu(A_j \cap C_k \cap D_m)}{\mu(A_j \cap D_m)} \frac{\mu(A_j \cap D_m)}{\mu(D_m)}.
				\]
				If $\mu(A_j \cap D_m) = 0$, then the above evaluates to zero anyway, so we ignore such terms. Therefore we have
				\begin{align*}
					H_\mu(\A \join \C \mid \D) &= -\sum_{j, k, m}{\mu(A_j \cap C_k \cap D_m) \log{\frac{\mu(A_j \cap D_m)}{\mu(D_m)}}} \\
						& \qquad - \sum_{j, k, m}{\mu(A_j \cap C_k \cap D_m) \log{\frac{\mu(A_j \cap C_k \cap D_m)}{\mu(A_j \cap D_m)}}} \\
						&= -\sum_{j, m}{\mu(A_j \cap D_m) \log{\frac{\mu(A_j \cap D_m)}{\mu(D_m)}}} + H_\mu(\C \mid \A \join \D) \\
						&= H_\mu(\A \mid \D) + H_\mu(\C \mid \A \join \D).
				\end{align*}
			\item We put $\D = \{X, \emptyset\}$ in \ref{walters-thm-4.3:1}. Then by the above remark the result follows immediately.
			\item Suppose that $\A \subset \C$. Then
				\begin{align*}
					H_\mu(\C \mid \D) &= H_\mu(\A \join \C \mid \D) & \text{(since } \A \subset \C) \\
						&= H_\mu(\A \mid \D) + H_\mu(\C \mid \A \join \D) & \text{(by \ref{walters-thm-4.3:1})} \\
						&\geq H_\mu(\A \mid \D).
				\end{align*}
			\item As with \ref{walters-thm-4.3:2}, we put $\D = \{X, \emptyset\}$ in \ref{walters-thm-4.3:3}.
			\item Suppose that $\C \subset \D$ and fix $j, k$. We have
				\[
					\sum_{m}{\frac{\mu(D_m \cap C_k)}{\mu(C_k)}} = 1
				\]
				and so we may apply Theorem \ref{thm:walters-4-2-xlogx-convex}. So with $f(x) = x\log{x}$, we have
				\begin{equation} \label{fml:walters-4-3-5-ineq}
					f\left(\sum_{m}{\frac{\mu(D_m \cap C_k)}{\mu(C_k)} \frac{\mu(A_j \cap D_m)}{\mu(D_m)}}\right) \leq \sum_{m}{\frac{\mu(D_m \cap C_k)}{\mu(C_k)} f\left(\frac{\mu(A_j \cap D_m)}{\mu(D_m)}\right)}.
				\end{equation}
				Since $\C \subset \D$, we have
				\begin{align*}
					f\left(\sum_{m}{\frac{\mu(D_m \cap C_k)}{\mu(C_k)} \frac{\mu(A_j \cap D_m)}{\mu(D_m)}}\right) &= f\left(\frac{\mu(A_j \cap C_k)}{\mu(C_k)}\right) \\
						&= \frac{\mu(A_j \cap C_k)}{\mu(C_k)} \log{\frac{\mu(A_j \cap C_k)}{\mu(C_k)}}.
				\end{align*}
				Then multiplying \eqref{fml:walters-4-3-5-ineq} by $\mu(C_k)$ and then summing over $j, k$, we get
				\begin{align*}
					-H_\mu(\A \mid \C) &= \sum_{j, k}{\mu(A_j \cap C_k) \log{\frac{\mu(A_j \cap C_k)}{\mu(C_k)}}} \\
						&\leq \sum_{j, k, m}{\mu(D_m \cap C_k) \frac{\mu(A_j \cap D_m)}{\mu(D_m)} \log{\frac{\mu(A_j \cap D_m)}{\mu(D_m)}}} \\
						&= \sum_{j, m}{\mu(A_j \cap D_m) \log{\frac{\mu(A_j \cap D_m)}{\mu(D_m)}}} \\
						&= -H_\mu(\A \mid \D).
				\end{align*}
				Hence $H_\mu(\A \mid \C) \geq H_\mu(\A \mid \D)$.
			\item We put $\C = \{X, \emptyset\}$ in \ref{walters-thm-4.3:5}.
			\item We have
				\begin{align*}
					H_\mu(\A \join \C \mid \D) &= H_\mu(\A \mid \D) + H_\mu(\C \mid \A \join \D) & \text{(by \ref{walters-thm-4.3:1})} \\
						&\leq H_\mu(\A \mid \D) + H_\mu(\C \mid \D) & \text{(by \ref{walters-thm-4.3:5})}.
				\end{align*}
			\item We put $\D = \{X, \emptyset\}$ in \ref{walters-thm-4.3:7}.
			\item This follows since $T$ is measure-preserving and by the definition conditional entropy.
			\item This is also immediate from the definitions.
		\end{enumerate}
	\end{proof}
\end{theorem}

Using conditional expectation, we can define conditional entropy for a finite sub-$\sigma$-algebra $\A$ of $\B$ given an arbitrary (not necessarily finite) sub-$\sigma$-algebra $\C$ of $\B$. We first suppose that $\C$ is finite so that $\alpha(\C) = \{C_1, \dots, C_q\}$, and also let $\alpha(\A) = \{A_1, \dots, A_p\}$. Note that
\[
		E_\mu(\chi_{A_j} \mid \C)(x) = \sum_{k = 1}^{q}{\int_{C_k}{\chi_{A_j}\ d\mu}\frac{\chi_{C_k}(x)}{\mu(C_k)}}.
\]

Then
\begin{align*}
	H_\mu(\alpha(\A) \mid \alpha(\C)) = H_\mu(\A \mid \C) &= -\sum_{j = 1}^{p}{\sum_{k = 1}^{q}{\mu(A_j \cap C_k) \log{\frac{\mu(A_j \cap C_k)}{\mu(C_k)}}}} \\
		&= -\sum_{j = 1}^{p}{\int{\chi_{A_j} \log{E_\mu(\chi_{A_j} \mid \C)}\ d\mu}} \\
		&= -\int{\sum_{j = 1}^{p}{E_\mu(\chi_{A_j} \mid \C) \log{E_\mu(\chi_{A_j} \mid \C)}}\ d\mu}.
\end{align*}

We can therefore make the following definition for countable sub-$\sigma$-algebras $\C$ of $\B$.

\begin{definition}
	Let $(X, \B, \mu)$ be a probability space. Suppose that $\A$ is a finite sub-$\sigma$-algebra of $\B$ and that $\C$ is an \emph{arbitrary} sub-$\sigma$-algebra of $\B$. Denote the partition $\alpha(\A) = \{A_1, \dots, A_p\}$. The \key{conditional entropy} of $\alpha$ given $\C$ is given by
	\[
		H_\mu(\alpha(\A) \mid \alpha(\C)) = H_\mu(\A \mid \C) := -\int{\sum_{j = 1}^{p}{\mu(A_j \mid \C) \log{\mu(A_j \mid \C)}}\ d\mu}.
	\]
\end{definition}

\begin{lemma} \label{lem:walters-4-6}
	Suppose that $\A_1 \subset \A_2 \subset \dots \subset \A_n \subset \dots$ is an increasing sequence of sub-$\sigma$-algebras of $\B$, and write $\A := \bigjoin_{n = 1}^\infty{\A_n}$. Then for all $f \in L^2(X, \B, \mu)$ we have $\|E_\mu(f \mid \A_n) - E_\mu(f \mid \A)\|_2 \to 0$, as $n \to \infty$.
	\begin{proof}
		By definition, the operator $E_\mu(\seedot \mid \A_n)$ maps functions from from $L^2(X, \B, \mu)$ to $L^2(X, \A_n, \mu)$. We let $A \in \A$ and choose a sequence $A_n \in \A_n$ such that $\mu(A_n \symdiff A) \to 0$, as $n \to +\infty$. (This is possible because $\A_n$ is an increasing sequence.)
		
		Since $E_\mu(\chi_A \mid \A_n)$ is a best approximation to $\chi_A$ in $L^2(X, \A_n, \mu)$, we have
		\[
			\|E_\mu(\chi_A \mid \A_n) - \chi_A\|_2^2 \leq \|\chi_{A_n} - \chi_A\|_2^2 = \mu(A_n \symdiff A) \to 0,
		\]
		as $n \to +\infty$.
		
		The set of all finite linear combinations of characteristic functions are dense in $L^2(X, \A, \mu)$ and so for all $g \in L^2(X, \A, \mu)$, we have
		\begin{equation} \label{fml:lem-4-6-star}
			\|E_\mu(g \mid \A_n) - g\|_2 \to 0,
		\end{equation}
		as $n \to +\infty$. Therefore if $f \in L^2(X, \B, \mu)$, then by property \ref{cond-exp:5} on page \pageref{cond-exp:5}, we have $E_\mu(E_\mu(f \mid \A) \mid \A_n) = E_\mu(f \mid \A_n)$ because $\A_n \subset \A$ for all $n \geq 1$. Hence by \eqref{fml:lem-4-6-star} we have
		\[
			\|E_\mu(f \mid \A_n) - E_\mu(f \mid \A)\|_2 = \|E_\mu(E_\mu(f \mid \A) \mid \A_n) - E_\mu(f \mid \A)\|_2 \to 0,
		\]
		as $n \to +\infty$, as required.
	\end{proof}
\end{lemma}

We also have the following theorem.

\begin{theorem}[Increasing Martingale Theorem] \label{thm:increasing-martingale}
	Suppose that $\A_1 \subset \A_2 \subset \dots \subset \A_n \subset \dots$ is an increasing sequence of sub-$\sigma$-algebras of $\B$ such that $\A_n \to \A$, as $n \to +\infty$. Then for all $f \in L^1(X, \B, \mu)$ we have
	\begin{enumerate}
		\item $E_\mu(f \mid \A_n) \to E_\mu(f \mid \A)$ $\mu$-almost everywhere, as $n \to +\infty$, and
		\item $E_\mu(f \mid \A_n) \to E_\mu(f \mid \A)$ in $L_1$, as $n \to +\infty$.
	\end{enumerate}
\end{theorem}

Note that \thref{thm:walters-4.3} was given in terms of \emph{finite} sub-$\sigma$-algebras and hence finite partitions. By the following theorem, we can in fact extend these results for \emph{countable} sub-$\sigma$-algebras and partitions.

\begin{theorem} \label{thm:walters-4-7}
	Suppose that $\A$ is a \emph{finite} sub-$\sigma$-algebra of $\B$. Furthermore, suppose that $\C_1 \subset \C_2 \subset \dots \subset \C_n \subset \dots$ is an increasing sequence of sub-$\sigma$-algebras of $\B$, and put $\C:= \bigvee_{n = 1}^\infty{\C_n}$. Then $H_\mu(\A \mid \C_n) \to H_\mu(\A \mid \C)$, as $n \to +\infty$.
	\begin{proof}
		Let $\alpha(\A) = \{A_1, \dots, \A_k\}$. By \thref{lem:walters-4-6}, $\|E_\mu(\chi_{A_j} \mid \C_n) - E_\mu(\chi_{A_j} \mid \C)\|_2 \to 0$, as $n \to +\infty$ for $j = 1, \dots, k$. So $E_\mu(\chi_{A_j} \mid \C_n)$ converges in measure to $E_\mu(\chi_{A_j} \mid \C)$, i.e. given $\varepsilon > 0$, we have that
		\[
			\lim_{n \to +\infty}{\mu\left\{x \in X \midmid \left|E_\mu(\chi_{A_j} \mid \C_n)(x) - E_\mu(\chi_{A_j} \mid \C)(x)\right| \geq \varepsilon\right\}} = 0.
		\]
		So it is clear that $-\sum_{j = 1}^k{E_\mu(\chi_{A_j} \mid \C_n) \log{E_\mu(\chi_{A_j} \mid \C_n)}}$ also converges in measure to $-\sum_{j = 1}^k{E_\mu(\chi_{A_j} \mid \C) \log{E_\mu(\chi_{A_j} \mid \C)}}$.
		
		Since $E_\mu(\seedot \mid \C)$ is a positive linear operator and since $\sum_{j = 1}^k{\chi_{A_j}} = 1$, we have $0 \leq E_\mu(\chi_{A_j} \mid \C)(x) \leq 1$ for $\mu$-almost every $x$. Hence
		\begin{align*}
			-\sum_{j = 1}^k{\mu(A_j \mid \C)(x) \log{\mu(A_j \mid \C)(x)}} &= -\sum_{j = 1}^k{E_\mu(\chi_{A_j} \mid \C)(x) \log{E_\mu(\chi_{A_j} \mid \C)(x)}} \\
				&\leq k \max_{t \in [0, 1]}(-t \log{t}) \\
				&= ke.
		\end{align*}
		So all functions of this form are bounded by $ke$ and hence converge in $L^1(\mu)$. Therefore, $H_\mu(\A \mid \C_n) \to H_\mu(\A \mid \C)$, as $n \to +\infty$.
	\end{proof}
\end{theorem}

As a result of this theorem, given a countable (not necessarily finite) sub-$\sigma$-algebra $\C$, we can find an increasing sequence $\C_1 \subset \C_2 \subset \dots \subset \C_n \subset \dots$ such that $\C_n \to \C$, as $n \to +\infty$. We then apply \thref{thm:walters-4-7} and we see that any result involving finite sub-$\sigma$-algebras can be extended for countable sub-$\sigma$-algebras.

\section{\texorpdfstring{\sloppy Entropy of measure-preserving transformations}{Entropy of measure-preserving transformations}}
So far, we have be focusing exclusively on the entropy of partitions and sub-$\sigma$-algebras, but we can now introduce a measure-preserving transformation $T : X \to X$. We can think of $T$ as the passing of a day in time, and so $H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\alpha}}\right)$ is the average information we gain after $n$ days. Given a partition $\alpha$, it is natural to define the entropy of $T$ by the average information we obtain \emph{per day}. First, we need to ensure that this is well-defined.

\begin{theorem} \label{thm:walters-4-9}
	Let $(a_n)_{n = 1}^\infty$ be a sequence of real numbers such that $a_{n + p} \leq a_n + a_p$ for all $n, p \geq 1$. Then $\lim_{n \to +\infty}(a_n / n)$ exists and equals $\inf_{n \geq 1}(a_n / n)$.
	
	This limit could be $-\infty$, but if $a_n$ is bounded below then, by the properties of the sequence, the limit is non-negative.
	\begin{proof}
		Fix $p \geq 1$. We can write $n = kp + j$ for some $0 \leq j < p$, and then
		\[
			\frac{a_n}{n} = \frac{a_{kp + j}}{kp + j} \leq \frac{a_j}{kp} + \frac{a_{kp}}{kp} \leq \frac{a_j}{kp} + \frac{ka_p}{kp} = \frac{a_j}{kp} + \frac{a_p}{p}.
		\]
		We have that $k \to +\infty$, as $n \to +\infty$, and so
		\[
			\frac{a_j}{kp} \to 0,
		\]
		as $n \to +\infty$. Putting the above results together, we have
		\[
			\limsup_{n \to +\infty}{\frac{a_n}{n}} \leq \frac{a_p}{p}.
		\]
		Since $p$ is fixed, we have
		\[
			\limsup_{n \to +\infty}{\frac{a_n}{n}} \leq \inf_{p \geq 1}{\frac{a_p}{p}}.
		\]
		On the other hand, it is clear that
		\[
			\inf_{p \geq 1}{\frac{a_p}{p}} \leq \liminf_{n \to +\infty}{\frac{a_n}{n}}.
		\]
		Therefore
		\[
			\lim_{n \to +\infty}{\frac{a_n}{n}}
		\]
		exists and is equal to the infimum.
	\end{proof}
\end{theorem}

\begin{corollary} \label{cor:walters-4-9-1}
	Let $T : X \to X$ be a measure-preserving transformation and suppose that $\A$ is a finite sub-$\sigma$-algebra of $\B$. Then
	\[
		\lim_{n \to +\infty}{\frac{1}{n} H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}}\right)}
	\]
	exists.
	\begin{proof}
		Let the sequence $(a_n)_{n = 1}^\infty$ be defined by $a_n = H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}}\right) \geq 0$. For any $n, p \geq 1$ we have
		\begin{align*}
			a_{n + p} &= H_\mu\left(\bigjoin_{j = 0}^{n + p - 1}{T^{-j}{\A}}\right) \\
				&\leq H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}}\right) + H_\mu\left(\bigjoin_{j = n}^{n + p - 1}{T^{-j}{\A}}\right) & \text{(by \thref{thm:walters-4.3} \ref{walters-thm-4.3:8})} \\
				&= a_n + H_\mu\left(\bigjoin_{j = 0}^{p - 1}{T^{-j}{\A}}\right) & \text{(by \thref{thm:walters-4.3} \ref{walters-thm-4.3:10})} \\
				&= a_n + a_p.
		\end{align*}
		By \thref{thm:walters-4-9}, the limit of $a_n$ exists and hence
		\[
			\lim_{n \to +\infty}{\frac{1}{n} H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}}\right)}
		\]
		exists.
	\end{proof}
\end{corollary}

This ensures that the following definition is well-defined.

\begin{definition}
	Let $(X, \B, \mu, T)$ be a measure-preserving transformation of a probability space and let $\alpha$ be a finite partition of $X$. The \key{entropy of $T$ with respect to $\alpha$} is defined
	\[
		h_\mu(T, \A(\alpha)) = h_\mu(T, \alpha) := \lim_{n \to +\infty}{\frac{1}{n} H_\mu\left(\bigjoin_{j = 0}^{n - 1}{}T^{-j}{\alpha}\right)}.
	\]
\end{definition}

\begin{theorem} \label{thm:walters-4.12}
	Let $\A$, $\C$ be finite sub-algebras of $\B$ and let $T$ be a measure-preserving transformation of a probability space $(X, \B, \mu)$ Then
	\begin{enumerate}
		\item $h_\mu(T, \A) \leq H_\mu(\A)$, \label{walters:thm-4-12:1}
		\item $h_\mu(T, \A \join \C) \leq h_\mu(T, \A) + h_\mu(T, \C)$, \label{walters:thm-4-12:2}
		\item if $\A \subset \C$ then $h_\mu(T, \A) \leq h_\mu(T, \C)$, \label{walters:thm-4-12:3}
		\item $h_\mu(T, \A) \leq h_\mu(T, \C) + H_\mu(\A \mid \C)$, \label{walters:thm-4-12:4}
		\item $h_\mu(T, T^{-1}{\A}) = h_\mu(T, \A)$, \label{walters:thm-4-12:5}
		\item if $m \geq 1$ then $h_\mu(T, \A) = h_\mu\left(T, \bigjoin\limits_{j = 0}^{m - 1}{T^{-j}{\A}}\right)$, \label{walters:thm-4-12:6}
		\item if $T$ is invertible and $m \geq 1$ then $h_\mu(T, \A) = h_\mu\left(T, \bigjoin\limits_{j = -m}^m{T^{-j}{\A}}\right)$. \label{walters:thm-4-12:7}
	\end{enumerate}
	\begin{proof} \hfill
		\begin{enumerate}
			\item For all $n \geq 1$,
				\begin{align*}
					\frac{1}{n} H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}}\right) &\leq \frac{1}{n} \sum_{j = 0}^{n - 1}{H_\mu({T^{-j}{\A}})} & \text{(by \thref{thm:walters-4.3} \ref{walters-thm-4.3:8})} \\
						&\leq \frac{1}{n} \sum_{j = 0}^{n - 1}{H_\mu(\A)} & \text{(by \thref{thm:walters-4.3} \ref{walters-thm-4.3:10})} \\
						&= H_\mu(\A).
				\end{align*}
			\item We have
				\begin{align*}
					H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}(\A \join \C)}\right) &= H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}} \join \bigjoin_{j = 0}^{n - 1}{T^{-j}{\C}}\right) \\
						&\leq H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}}\right) + H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\C}}\right)
				\end{align*}
				by \thref{thm:walters-4.3} \ref{walters-thm-4.3:8}.
			\item Since $T$ preserves set theoretic operations, if $\A \subset \C$, then for all $n \geq 1$ we have
				\[
					\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}} \subset \bigjoin_{j = 0}^{n - 1}{T^{-j}{\C}}.
				\]
				Applying \thref{thm:walters-4.3} \ref{walters-thm-4.3:4} we get
				\[
					H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}}\right) \leq H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\C}}\right).
				\]
			\item We have
				\begin{align*}
					H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}}\right) &\leq H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}} \join \bigjoin_{j = 0}^{n - 1}{T^{-j}{\C}}\right) \\ & \hspace{60mm} \text{(by \thref{thm:walters-4.3} \ref{walters-thm-4.3:4})} \\
						&= H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\C}}\right) + H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}} \midmid \bigjoin_{j = 0}^{n - 1}{T^{-j}{\C}}\right) \\ & \hspace{60mm} \text{(by \thref{thm:walters-4.3} \ref{walters-thm-4.3:2}).}
				\end{align*}
				By applying \thref{thm:walters-4.3} \ref{walters-thm-4.3:7} repeatedly, we have
				\begin{align*}
					H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}} \midmid \bigjoin_{j = 0}^{n - 1}{T^{-j}{\C}}\right) &\leq \sum_{j = 0}^{n - 1}{H_\mu\left(T^{-j}{\A} \midmid \bigjoin_{k = 0}^{n - 1}{T^{-k}{\C}}\right)} \\
						&\leq \sum_{j = 0}^{n - 1}{H_\mu\left(T^{-j}{\A} \midmid T^{-j}{\C}\right)} & \text{(by \thref{thm:walters-4.3} \ref{walters-thm-4.3:5})} \\
						&= nH_\mu(\A \mid \C) & \text{(by \thref{thm:walters-4.3} \ref{walters-thm-4.3:9}).}
				\end{align*}
				Combining this with the previous result, we have
				\begin{align*}
					h_\mu(T, \A) &\leq \lim_{n \to +\infty}{\left[\frac{1}{n} H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\C}}\right) + nH_\mu(\A \mid \C)\right]} \\
						&= h_\mu(T, \C) + H_\mu(\A \mid \C).
				\end{align*}
			\item By \thref{thm:walters-4.3} \ref{walters-thm-4.3:10} we have
				\begin{align*}
					h_\mu(T, T^{-1}{\A}) &= \lim_{n \to +\infty}{\frac{1}{n} H_\mu\left(\bigjoin_{j = 1}^{n - 1}{T^{-j}{\A}}\right)} \\
						&= \lim_{n \to +\infty}{\frac{1}{n} H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}}\right)} \\
						&= h_\mu(T, \A)
				\end{align*}
			\item Let $m \geq 1$ be fixed. Then
				\begin{align*}
					h_\mu\left(T, \bigjoin_{j = 0}^{m}{T^{-j}{\A}}\right) &= \lim_{n \to +\infty}{\frac{1}{n} H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}\left(\bigjoin_{k = 0}^{m}{T^{-k}{\A}}\right)}\right)} \\
						&= \lim_{n \to +\infty}{\frac{1}{n} H_\mu\left(\bigjoin_{j = 0}^{n + m - 1}{T^{-j}{\A}}\right)} \\
						&= \lim_{n \to +\infty}{\left(\frac{m + n}{n}\right) \frac{1}{m + n} H_\mu\left(\bigjoin_{j = 0}^{n + m - 1}{T^{-j}{\A}}\right)} \\
						&= h_\mu(T, \A)
				\end{align*}
			\item Suppose that $T$ is invertible and fix $m \geq 1$. We have
				\begin{align*}
					h_\mu\left(T, \bigjoin_{j = -m}^{m}{T^{-j}{\A}}\right) &= h_\mu\left(T, \bigjoin_{j = 0}^{2m}{T^{-j}{\A}}\right) & \text{(by \ref{walters:thm-4-12:5})} \\
						&= h_\mu(T, \A) & \text{(by \ref{walters:thm-4-12:6}).}
				\end{align*}
		\end{enumerate}
	\end{proof}
\end{theorem}

There is an alternative definition for $h_\mu(T, \alpha)$ which is useful if we want to utilise results relating to conditional entropy.

\begin{theorem}
	Suppose that $(X, \B, \mu, T)$ is a measure-preserving transformation of a probability space and that $\alpha$ be a finite partition of $X$. The entropy of $T$ with respect to $\alpha$ (or $\A(\alpha)$) may also be given by
	\[
		h_\mu(T, \A(\alpha)) = h_\mu(T, \alpha) = H_\mu\left(\alpha \midmid \bigjoin_{j = 1}^\infty {T^{-j}{\alpha}}\right).
	\]
	
	\begin{proof}
		We follow the proof given in \cite[Lecture 24]{ergodic-lectures}.
		
		Let $\A := \A(\alpha)$. We have
		\begin{align*}
			H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}}\right) &= H_\mu\left(\bigjoin_{j = 1}^{n - 1}{T^{-j}{\A}}\right) + H_\mu\left(\A \midmid \bigjoin_{j = 1}^{n - 1}{T^{-j}{\A}}\right) & \text{(by \thref{thm:walters-4.3} \ref{walters-thm-4.3:2})} \\
				&= H_\mu\left(\bigjoin_{j = 0}^{n - 2}{T^{-j}{\A}}\right) + H_\mu\left(\A \midmid \bigjoin_{j = 1}^{n - 1}{T^{-j}{\A}}\right) & \text{(by \thref{thm:walters-4.3} \ref{walters-thm-4.3:10})}.
		\end{align*}
		By induction, this means that
		\begin{align*}
			\frac{1}{n} H_\mu\left(\bigjoin_{j = 0}^{n - 1}{T^{-j}{\A}}\right) &= \frac{1}{n}\left[H_\mu\left(\A \midmid \bigjoin_{j = 1}^{n - 2}{T^{-j}{\A}}\right) + H_\mu\left(\A \midmid \bigjoin_{j = 1}^{n - 3}{T^{-j}{\A}}\right)\right. \\
				& \left. \vphantom{\bigjoin_{j = 1}^{n - 2}{T^{-j}{\A}}} \qquad + \dots + H_\mu(\A \mid T^{-1}{\A}) + H_\mu(\A)\right].
		\end{align*}
		By \thref{thm:walters-4.3} \ref{walters-thm-4.3:5} we have
		\[
			H_\mu\left(\A \midmid \bigjoin_{j = 1}^{n - 1}{T^{-j}{\A}}\right) \leq H_\mu\left(\A \midmid \bigjoin_{j = 1}^{n - 2}{T^{-j}{\A}}\right) \leq \dots \leq H_\mu(\A).
		\]
		We may now apply \thref{thm:increasing-martingale} to get that
		\[
			H_\mu\left(\A \midmid \bigjoin_{j = 1}^{n - 1}{T^{-j}{\A}}\right) \to H_\mu\left(\A \midmid \bigjoin_{j = 1}^\infty{T^{-j}{\A}}\right),
		\]
		as $n \to +\infty$, and therefore
		\begin{align*}
			h_\mu(T, \A) = \lim_{n \to +\infty}{\frac{1}{n} H_\mu\left(\bigjoin_{j = 0}^{n - 1}{}T^{-j}{\A}\right)} &= H_\mu\left(\A \midmid \bigjoin_{j = 1}^\infty{T^{-j}{\A}}\right).
		\end{align*}
	\end{proof}
\end{theorem}

Finally, we can think of the entropy of a measure-preserving transformation $T$, regardless of the choice of partition, to be maximal average information we can gain per day. We state this formally as follows.

\begin{definition}
	Let $(X, \B, \mu, T)$ be a measure-preserving transformation of a probability space. The \key{entropy of $T$} is defined
	\[
		h_\mu(T) := \sup_{\alpha}{h_\mu(T, \alpha)} = \sup_{\A}{h_\mu(T, \A)},
	\]
	where the supremum is taken over all finite measurable partitions $\alpha$ or finite sub-$\sigma$-algebras of $\B$, respectively.
\end{definition}

\begin{remark}
	If $T = \id_X$, then $h(T) = 0$. In general, if $h(T) = 0$, then $h(T, \alpha) = 0$ for all finite partitions $\alpha$. This means that we don't obtain `much' new information each day, i.e. $\bigvee_{j = 0}^{n - 1}{T^{-j}{\alpha}}$ doesn't change `much', as $n \to +\infty$. One such measure-preserving transformation which has this property is the identity transformation on $X$.
\end{remark}

\begin{theorem}
	Entropy is a conjugacy invariant and hence is also an isomorphism invariant.
	\begin{proof}
		Let $(X_1, \B_1, \mu_1, T_1), (X_2, \B_2, \mu_2, T_2)$ be measure-preserving transformations of probability spaces. Let $\Phi : (\tilde{B}_2, \tilde{\mu}_2) \to (\tilde{B}_1, \tilde{\mu}_1)$ be an isomorphism of measure algebras such that $\Phi \circ \tilde{T}_2^{-1} = \tilde{T}_1^{-1} \circ \Phi$. We aim to show that $h_{\mu_1}(T_1) = h_{\mu_2}(T_2)$.
		
		Let $\A_2$ be an arbitrary finite sub-$\sigma$-algebra of $\B_2$ and write $\alpha(\A_2) = \{A_1, \dots, A_r\}$. Since $\Phi$ is an isomorphism of measure algebras, we can choose $C_j \in \B_1$ such that $\tilde{C}_j = \Phi(\tilde{A}_j)$. Using this, we define $\gamma := \{C_1, \dots, C_r\}$, which we see is a partition of $(X_1, \B_1, \mu_1)$. We write $\A_1 := \A(\gamma)$.
		
		For any $(q_0, q_1, \dots, q_{n - 1})$, where $q_j \in \{1, \dots, r\}$ for each $j$, we have
		\begin{align*}
			\Phi\left(\bigcap_{j = 0}^{n - 1}{(T_2^{-j} A_{q_j})^\sim}\right) &= \Phi\left(\bigcap_{j = 0}^{n - 1}{\tilde{T}_2^{-j} \tilde{A}_{q_j}}\right) \\
				&= \bigcap_{j = 0}^{n - 1}{\tilde{T}_1^{-j} \Phi(\tilde{A}_{q_j})} \\
				&= \bigcap_{j = 0}^{n - 1}{\tilde{T}_1^{-j} \tilde{C}_{q_j}} \\
				&= \bigcap_{j = 0}^{n - 1}{(T_1^{-j} C_{q_j})^\sim}.
		\end{align*}
		Hence the sets $\bigcap_{j = 0}^{n - 1}{(T_2^{-j} A_{q_j})^\sim}$ and $\bigcap_{j = 0}^{n - 1}{(T_1^{-j} C_{q_j})^\sim}$ have the same measure. Recall that the entropy of a partition is completely determined by the measure of the elements in the partition. This means that
		\[
			H_{\mu_1}\left(\bigjoin_{j = 0}^{n - 1}{T_1^{-j}{\A_1}}\right) = H_{\mu_2}\left(\bigjoin_{j = 0}^{n - 1}{T_2^{-j}{\A_2}}\right),
		\]
		and hence $h_{\mu_1}(T_1, \A_1) = h_{\mu_2}(T_2, \A_2)$. Since $\A_2$ was chosen to be an arbitrary sub-$\sigma$-algebra of $\B_1$, this means that $h_{\mu_1}(T_1) \geq h_{\mu_2}(T_2)$.
		
		We repeat the proof, but choose an arbitrary finite sub-$\sigma$-algebra of $\B_1$ to get the reverse inequality $h_{\mu_1}(T_1) \leq h_{\mu_2}(T_2)$, and hence $h_{\mu_1}(T_1) = h_{\mu_2}(T_2)$.
	\end{proof}
\end{theorem}

\section{Calculating \texorpdfstring{$h_\mu(T)$}{h(T)}}
Recall that the entropy of a measure-preserving transformation $T$ is defined $h_\mu(T) := \sup_{\A}{h_\mu(T, \A)}$, where the supremum is taken over all finite sub-$\sigma$-algebras of $\B$ (or, equivalently, over all finite partitions of $(X, \B, \mu)$). Of course, it is difficult to consider all finite partitions, so we want to find criteria which guarantee that $h_\mu(T) = h_\mu(T, \A)$ instead.

One key result is the \key{Kolmogorov-Sinai Theorem}. We will prove this in \thref{thm:kolmogorov-sinai}, but to do this we need some preliminary results.

\begin{lemma} \label{lem:walters-4-15}
	Let $r \in \naturals$ be fixed and let $\varepsilon > 0$ be given. Then there exists $\delta > 0$ such that, if we have two partitions of $r$ sets $\alpha = \{A_1, \dots, A_r\}$, $\gamma = \{C_1, \dots, C_r\}$ of $(X, \B, \mu)$ such that
	\[
		\sum_{j = 1}^r{\mu(A_j \symdiff C_j)} < \delta,
	\]
	then we have $H_\mu(\alpha \mid \gamma) + H_\mu(\gamma \mid \alpha) < \varepsilon$.
	
	\begin{proof}
		Let $\varepsilon > 0$ be given. We choose $\delta > 0$ such that $\delta < 1 / 4$ and
		\[
			-r(r - 1) \delta \log{\delta} - (1 - \delta) \log(1 - \delta) < \frac{\varepsilon}{2}.
		\]
		
		Let $\beta$ be the partition of $(X, \B, \mu)$ consisting of the sets of the form $A_j \cap C_k$, where $j \neq k$, and the set $\bigcup_{j = 1}^r{A_j \cap C_j}$. It is then clear that $\alpha \join \gamma = \gamma \join \beta$. For $j \neq k$, we also have
		\[
			A_j \cap C_k \subset \bigcup_{n = 1}^r{A_n \symdiff C_n}.
		\]
		This gives, by the hypothesis, $\mu(A_j \cap C_k) < \delta$ for $j \neq k$. By the definition of symmetric difference, we also have
		\[
			\mu\left(\bigcup_{j = 1}^r{A_j \cap C_j}\right) > 1 - \delta.
		\]
		Hence
		\begin{align*}
			H_\mu(\beta) &= -\sum_{j \neq k}{\mu(A_j \cap C_k) \log{\mu(A_j \cap C_k)}} - \mu\left(\bigcup_{j = 1}^r{A_j \cap C_j}\right) \log{\mu\left(\bigcup_{j = 1}^r{A_j \cap C_j}\right)} \\
				&< -r(r - 1) \delta \log{\delta} - (1 - \delta) \log(1 - \delta) \\
				&< \frac{\varepsilon}{2}.
		\end{align*}
		We therefore have
		\begin{align*}
			H_\mu(\gamma) + H_\mu(\alpha \mid \gamma) &= H_\mu(\alpha \join \gamma) & \text{(by \thref{thm:walters-4.3} \ref{walters-thm-4.3:2})} \\
				&= H_\mu(\gamma \join \beta) \\
				&\leq H_\mu(\gamma) + H_\mu(\beta) & \text{(by \thref{thm:walters-4.3} \ref{walters-thm-4.3:8})} \\
				&< H_\mu(\gamma) + \frac{\varepsilon}{2},
		\end{align*}
		and hence $H_\mu(\alpha \mid \gamma) < \varepsilon / 2$.
		
		We repeat this argument using $\alpha \join \gamma = \alpha \join \beta$ to get $H_\mu(\gamma \mid \alpha) < \varepsilon / 2$. Combining these two results, we get $H_\mu(\alpha \mid \gamma) + H_\mu(\gamma \mid \alpha) < \varepsilon$.
	\end{proof}
\end{lemma}

\begin{theorem} \label{thm:walters-4-16}
	Suppose that $\C$ is a finite sub-$\sigma$-algebra of $\B$ and that $\B_0$ is an algebra such that $\B(\B_0) = \B$ $\mu$-almost everywhere. Then given any $\varepsilon > 0$, there exists a finite algebra $\D \subset \B_0$ such that
	\[
		H_\mu(\D \mid \C) + H_\mu(\C \mid \D) < \varepsilon.
	\]
	
	\begin{proof}
		Let $\varepsilon > 0$ be given and write $\alpha(\C) = \{C_1, \dots, C_r\}$. We choose $\delta > 0$ as in \thref{lem:walters-4-15}, where $r, \varepsilon$ here are as in the lemma. It suffices to show that, for each $\tau > 0$, there exists a partition $\D = \{D_1, \dots, D_r\}$, where $D_j \in \B_0$ and $\mu(C_j \symdiff D_j) < \tau$, for all $j = 1, \dots, r$. This is because we may then choose $\tau$ such that $r\tau \leq \delta$ and then apply \thref{lem:walters-4-15}.
		
		To begin, we choose $\lambda > 0$ such that $\lambda(r - 1)[1 + r(r - 1)] < \tau$. For each $j = 1, \dots, r$, choose $B_j \in \B_0$ such that $\mu(C_j \symdiff B_j) < \lambda$. Now if $j \neq k$, then $B_j \cap B_k \subset (B_j \symdiff C_j) \cup (\B_j \symdiff C_j)$. It follows that $\mu(B_j \cap B_k) < 2\lambda$. We let $N := \bigcup_{j \neq k}{(B_j \cap B_k)}$, so that $\mu(N) < r(r - 1)\lambda$.
		
		Now for $j = 1, \dots, r - 1$ we define $D_j = B_j \setminus N$, and $D_r = X \setminus \bigcup_{j = 1}^{r - 1}{D_j}$. This clearly defines a partition $\D := \{D_1, \dots, D_r\}$ of $X$, and each $D_j \in \B_0$, since $\B_0$ is an algebra (i.e. is closed under finite unions and complementation).
		
		If $j < r$, then $D_j \symdiff C_j \subset (B_j \symdiff C_j) \cup N$. Then by countable subadditivity,
		\begin{align*}
			\mu(D_j \symdiff C_j) &\leq \mu(B_j \symdiff C_j) + \mu(N) \\
				&< \lambda + r(r - 1)\lambda \\
				&= \lambda[1 + r(r - 1)] \\
				&< \tau.
		\end{align*}
		For the last part of $\D$, we use the fact that $D_r \symdiff C_r \subset \bigcup_{j = 1}^{r - 1}{(D_j \symdiff C_j)}$. Then $\mu(D_r \symdiff C_r) < (r - 1)\lambda[1 + r(r - 1)] < \tau$.
	\end{proof}
\end{theorem}

\begin{corollary} \label{cor:walters-4-16-1}
	Let $\A_1 \subset \A_2 \subset \dots \subset \A_n \subset \dots$ be an increasing sequence of finite sub-algebras of $\B$. Suppose that $\C$ is a finite sub-algebra of $\B$ such that $\C \subset \bigjoin_{n \geq 1}{\A_n}$ $\mu$-almost everywhere. Then $H_\mu(\C \mid \A_n) \to 0$, as $n \to +\infty$.
	
	Note: We say that $\A \subset \C$ $\mu$-almost everywhere if, for all $A \in \A$, there exists $C \in \C$ such that $\mu(A \symdiff C) = 0$.
	
	\begin{proof}
		Let $\varepsilon > 0$ be given. Write $\B_0 := \bigcup_{n = 1}^\infty{\A_n}$ so that $\B_0$ is an algebra. Since $\C \subset \bigjoin_{n \geq 1}{\A_n}$ $\mu$-almost everywhere, we have $\C \subset \B(\B_0)$ $\mu$-almost everywhere. By \thref{thm:walters-4-16}, there exists a finite sub-algebra $\D_\varepsilon$ of $\B_0$ such that $H_\mu(\C \mid \D_\varepsilon) < \varepsilon$.
		
		Since $\A_n$ is increasing and $\D_\varepsilon$ is finite, we have $\D_\varepsilon \subset \A_{n_0}$ for some $n_0 \in \naturals$. Then for all $n \geq n_0$, we have
		\[
			H_\mu(\C \mid \A_n) \leq H_\mu(\C \mid \A_{n_0}) \leq H_\mu(\C \mid \D_\varepsilon) < \varepsilon.
		\]
		Hence $H_\mu(\C \mid \A_n) \to 0$, as $n \to +\infty$.
	\end{proof}
\end{corollary}

We are ready to prove one of the main results of this chapter.

\begin{theorem}[Kolmogorov-Sinai Theorem] \label{thm:kolmogorov-sinai}
	Let $T : X \to X$ be an invertible measure-preserving transformation of a probability space $(X, \B, \mu)$. Suppose that $\A$ is a finite sub-$\sigma$-algebra of $\B$ such that
	\[
		\bigjoin_{n = -\infty}^\infty{T^{-n}{\A}} = \B
	\]
	$\mu$-almost everywhere. Then $h_\mu(T) = h_\mu(T, \A)$.
	
	\begin{proof}
		Let $\C$ be a finite sub-$\sigma$-algebra of $\B$. We want to show that $h_\mu(T, \C) \leq h_\mu(T, \A)$, i.e. $h_\mu(T, \A)$ achieves the supremum as in the definition of $h_\mu(T)$. We have
		\begin{align*}
			h_\mu(T, \C) &\leq h_\mu\left(T, \bigjoin_{j = -n}^n{T^{-j}{\A}}\right) + H_\mu\left(\C \midmid \bigjoin_{j = -n}^n{T^{-j}{\A}}\right) & \text{(by \thref{thm:walters-4.12} \ref{walters:thm-4-12:4})} \\
				&= h_\mu(T, \A) + H_\mu\left(\C \midmid \bigjoin_{j = -n}^n{T^{-j}{\A}} \right) & \text{(by \thref{thm:walters-4.12} \ref{walters:thm-4-12:7})}
		\end{align*}
		We let $\A_n := \bigjoin_{j = -n}^n{T^{-j}{\A}}$ so that $\A_n$ is an increasing sequence of finite sub-algebras of $\B$. Since $\bigjoin_{n = -\infty}^\infty{T^n{\A}} = \B$ $\mu$-almost everywhere, and $\C \subset \B$, we may apply \thref{cor:walters-4-16-1}. So $H_\mu(\C \mid \A_n) \to 0$, as $n \to +\infty$, and hence $h_\mu(T, \C) \leq h_\mu(T, \A)$.
	\end{proof}
\end{theorem}

There is a similar result which does not require that $T$ is invertible.

\begin{theorem} \label{thm:walters-4-18}
	Let $T : X \to X$ be a (not necessarily invertible) measure-preserving transformation of a probability space $(X, \B, \mu)$. Suppose that $\A$ is a finite sub-algebra of $\B$ such that
	\[
		\bigjoin_{n = 0}^\infty{T^{-n}{\A}} = \B
	\]
	$\mu$-almost everywhere. Then $h_\mu(T) = h_\mu(T, \A)$.
	
	\begin{proof}
		We repeat the same proof for \thref{thm:kolmogorov-sinai}, but replace $\bigjoin_{j = -n}^n{T^{-j}{\A}}$ with $\bigjoin_{j = 0}^n{T^{-j}{\A}}$ and apply \thref{thm:walters-4.12} \ref{walters:thm-4-12:6} instead of \ref{walters:thm-4-12:7}.
	\end{proof}
\end{theorem}

The following result gives a useful criterion for deciding if an invertible measure-preserving transformation has zero entropy.

\begin{corollary}
	Suppose that $T : X \to X$ is an invertible measure-preserving transformation of a probability space $(X, \B, \mu)$, and that
	\[
		\bigjoin_{n = 0}^\infty{T^{-n}{\A}} = \B
	\]
	$\mu$-almost everywhere for some finite sub-algebra $\A$ of $\B$. Then $h_\mu(T) = 0$.
	
	\begin{proof}
		By \thref{thm:walters-4-18}, we have
		\[
			h_\mu(T) = h_\mu(T, \A) = \lim_{n \to +\infty}{H_\mu\left(\A \midmid \bigjoin_{j = 1}^n{T^{-j}{\A}}\right)}.
		\]
		Since $T$ is a measure-preserving transformation, we have $\bigjoin_{j = 1}^\infty{T^{-j}{\A}} = T^{-1}{\B} = \B$ $\mu$-almost everywhere.
		
		We write $\A_n := \bigjoin_{j = 1}^\infty{T^{-j}{\A}}$, so that $\A_n$ is an increasing sequence of sub-algebras of $\B$, and $\bigjoin_{n = 1}^\infty{\A_n} = \B$ $\mu$-almost everywhere. In particular, for all $n \geq 1$ we have $\A \subset \A_n$ $\mu$-almost everywhere and so we may apply \thref{cor:walters-4-16-1}. So $H_\mu(\A \mid \A_n) \to 0$, as $n \to +\infty$. Hence $h_\mu(T) = 0$.
	\end{proof}
\end{corollary}

\section{Shifts of finite type} \label{sec:entropy:sft}
We now consider entropy for shifts of finite type, which we will be interested in later on. The following definitions appear in \cite{chazottes-maldonado:cbfee}.

Let $A$ be a finite alphabet and let $\Sigma := \{(x_j)_{j = 0}^\infty \mid x_j \in A\}$ denote the full (one-sided) shift. As usual, $\sigma : \Sigma \to \Sigma$ will be the (one-sided) shift map. For the sake of readability, if $(x_j)_{j = 0}^\infty \in \Sigma$, then we will write $x_m^n := (x_j)_{j = m}^n$.

In this setting, our measure-preserving transformation will always be $\sigma$ and so we are more interested in $\sigma$-invariant probability measures than the shift map itself. We will define the entropy of such measures in the remainder of this chapter and we will see that each definition is consistent with our previous discussion.

For the remainder of this section, we let $\alpha_k := \{[a_0^{k - 1}] \mid a_0^{k - 1} \in A^k\}$ denote the partition of $(\Sigma, \B, \nu)$ by cylinders of length $k$.

\begin{definition}
	Let $\nu$ be a $\sigma$-invariant probability measure on $\Sigma$. For shifts of finite type, for $k > 1$ we can define the \key{$k$-block entropy} of $\nu$ by
	\[
		H_k(\nu) := -\sum_{a_0^{k - 1} \in A^k}{\nu[a_0^{k - 1}] \log\nu[a_0^{k - 1}]},
	\]
	where the sum is taken over all sequences of length $k$.
\end{definition}

By definition, the entropy of $\alpha_k$ is
\[
	H_\nu(\alpha_k) = -\sum_{a_0^{k - 1} \in A^k}{\nu[a_0^{k - 1}] \log\nu[a_0^{k - 1}]} = H_k(\nu).
\]
In other words, $k$-block entropy is the entropy of the partition of $\Sigma$ by cylinders of length $k$.

\begin{definition}
	Let $\nu$ be a $\sigma$-invariant probability measure on $\Sigma$. The \key{entropy} of $\nu$ is given by
	\[
		h(\nu) := \lim_{k \to +\infty}{\frac{1}{k} H_k(\nu)}.
	\]
\end{definition}

Let $\A$ be the sub-algebra of $\B$ consisting of unions cylinders of length $1$. Then $\bigjoin_{k = 0}^\infty{\sigma^{-k}{\A}} = \B$ and hence \thref{thm:walters-4-18} applies. So
\[
	h_\nu(\sigma) = h_\nu(\sigma, \A) = \lim_{k \to +\infty}{\frac{1}{k} H_\nu\left(\bigjoin_{j = 0}^{k - 1}{\sigma^{-j}{\A}}\right)} = \lim_{k \to +\infty}{\frac{1}{k} H_k\left(\nu\right)} = h(\nu).
\]
Therefore this definition is consistent with our previous results.

\begin{definition}
	The \key{conditional $k$-block entropy}, where $k \geq 2$, is defined
	\[
		h_k(\nu) := -\sum_{a_0^{k - 1} \in A^k}{\nu[a_0^{k - 1}] \log{\frac{\nu[a_0^{k - 1}]}{\nu[a_0^{k - 2}]}}}.
	\]
\end{definition}

Note that $[a_0^{k - 1}] \subset [a_0^{k - 2}]$ and hence for $C \in \alpha_{k - 1}$,
\[
	[a_0^{k - 1}] \cap C =
	\begin{cases}
		\left[a_0^{k - 1}\right],	& \text{if } C = [a_0^{k - 2}]; \\
		\emptyset,	& \text{otherwise}.
	\end{cases}
\]
Then by the definition of conditional entropy we have
\begin{align*}
	H_\nu(\alpha_k \mid \alpha_{k - 1}) &= -\sum_{a_0^{k - 1} \in A^k}{\nu([a_0^{k - 2}] \cap [a_0^{k - 1}]) \log{\frac{[a_0^{k - 2}] \cap [a_0^{k - 1}]}{[a_0^{k - 2}]}}} \\
		&= -\sum_{a_0^{k - 1} \in A^k}{\nu[a_0^{k - 1}] \log{\frac{[a_0^{k - 1}]}{[a_0^{k - 2}]}}} \\
		&= h_k(\nu),
\end{align*}
and so the definitions are once again consistent. For $k \geq 2$, we also clearly have the relation
\[
	h_k(\nu) = H_k(\nu) - H_{k - 1}(\nu).
\]

Finally, we have \key{relative $k$-block entropy} which we will use later.

\begin{definition}
	For $k \geq 1$, the \key{$k$-block relative entropy} of a measure $\nu$ with respect to a measure $\mu$ is defined
	\[
		H_k(\nu \mid \mu) = \sum_{a_0^{k - 1} \in A^k}{\nu[a_0^{k - 1}] \log{\frac{\nu[a_0^{k - 1}]}{\mu[a_0^{k - 1}]}}}.
	\]
\end{definition}
